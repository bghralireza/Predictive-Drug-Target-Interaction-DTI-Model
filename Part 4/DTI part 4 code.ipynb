{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "05a20670-b7ba-4c8f-a4b6-a610519b818e",
   "metadata": {},
   "source": [
    "# Phase 4: Model Training and Optimization\n",
    "\n",
    "Phase 4 involves setting up the training loop, defining loss, selecting an optimizer, and implementing techniques like early stopping and learning rate \n",
    "scheduling.\n",
    "\n",
    "Here is the code implementation from scratch, integrating the components from Phase 3.\n",
    "\n",
    "##### Prerequisites\n",
    "\n",
    "We'll assume you have the DTIModel, DTIDataset, and custom_collate functions defined from Phase 3 and the train_data, val_data DataFrames from Phase 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1aeec42c-58c2-4e5f-be5e-a311cb8e897b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "from sklearn.metrics import roc_auc_score, average_precision_score\n",
    "import numpy as np\n",
    "import os\n",
    "import time\n",
    "\n",
    "# --- ASSUMED IMPORTS FROM PREVIOUS PHASES ---\n",
    "# from my_model_scripts import DTIModel, custom_collate, DTIDataset \n",
    "# from my_data_scripts import train_data, val_data \n",
    "# We'll define dummy values for demonstration\n",
    "\n",
    "# --- DUMMY SETUP (REPLACE WITH REAL DATA/MODEL) ---\n",
    "# Define dummy dimensions based on Phase 2 & 3\n",
    "DRUG_IN_FEAT = 71 # Example number of atom features from Phase 2\n",
    "TARGET_IN_FEAT = 21 # Amino acid alphabet size ('ACDEFGHIKLMNPQRSTVWXY')\n",
    "EMBEDDING_DIM = 128\n",
    "HIDDEN_DIM = 64\n",
    "GNN_LAYERS = 3\n",
    "CNN_KERNEL_SIZE = 8\n",
    "MAX_LEN = 1200\n",
    "BATCH_SIZE = 32\n",
    "LEARNING_RATE = 1e-4\n",
    "NUM_EPOCHS = 50\n",
    "\n",
    "# Set device\n",
    "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75fd84a2-2cdb-4c89-bd37-f8d6d3d669c0",
   "metadata": {},
   "source": [
    "### 1. Training Setup and Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43eb467d-389b-407d-a69b-f6ae75db2772",
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_training_components(drug_in_feat, target_in_feat, embedding_dim, hidden_dim, gnn_layers, cnn_kernel_size, lr):\n",
    "    \"\"\"Initializes the model, optimizer, loss function, and device.\"\"\"\n",
    "    \n",
    "    # 1. Initialize Model\n",
    "    model = DTIModel(\n",
    "        drug_in_feat=drug_in_feat,\n",
    "        target_in_feat=target_in_feat,\n",
    "        hidden_dim=hidden_dim,\n",
    "        gnn_layers=gnn_layers,\n",
    "        cnn_kernel_size=cnn_kernel_size,\n",
    "        embedding_dim=embedding_dim\n",
    "    ).to(DEVICE)\n",
    "    \n",
    "    # 2. Define Loss Function\n",
    "    # For binary classification, use Binary Cross-Entropy with Logits (BCEWithLogitsLoss) \n",
    "    # if the sigmoid is NOT in the model's forward pass, or standard BCELoss if it is.\n",
    "    # Since we used sigmoid in DTIModel, we use BCELoss.\n",
    "    criterion = nn.BCELoss() \n",
    "    \n",
    "    # 3. Define Optimizer\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr, weight_decay=1e-5) # Added L2 regularization\n",
    "    \n",
    "    # 4. Learning Rate Scheduler\n",
    "    # Reduces LR if validation loss plateaus\n",
    "    scheduler = ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=5, verbose=True)\n",
    "    \n",
    "    return model, criterion, optimizer, scheduler\n",
    "\n",
    "# --- Dummy Data Loader Setup (Replace with actual data) ---\n",
    "# train_dataset = DTIDataset(train_data, max_len=MAX_LEN) \n",
    "# val_dataset = DTIDataset(val_data, max_len=MAX_LEN) \n",
    "\n",
    "# train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, collate_fn=custom_collate)\n",
    "# val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False, collate_fn=custom_collate)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb7c67c3-6ae2-419b-9821-c12d67b6a747",
   "metadata": {},
   "source": [
    "### 2. Helper Functions for Evaluation and Early Stopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ae1c4bf-9894-4682-b701-e00d9ab6553b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, loader, criterion):\n",
    "    \"\"\"Evaluates the model on a given dataset loader.\"\"\"\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    all_labels = []\n",
    "    all_predictions = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for drug_batch, target_batch, labels in loader:\n",
    "            drug_batch = drug_batch.to(DEVICE)\n",
    "            target_batch = target_batch.to(DEVICE)\n",
    "            labels = labels.to(DEVICE).float().unsqueeze(1)\n",
    "            \n",
    "            predictions = model(drug_batch, target_batch)\n",
    "            loss = criterion(predictions, labels)\n",
    "            total_loss += loss.item() * len(labels)\n",
    "            \n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "            all_predictions.extend(predictions.cpu().numpy())\n",
    "\n",
    "    avg_loss = total_loss / len(loader.dataset)\n",
    "    \n",
    "    # Calculate key metrics (AUROC and AUPRC)\n",
    "    try:\n",
    "        auroc = roc_auc_score(all_labels, all_predictions)\n",
    "        auprc = average_precision_score(all_labels, all_predictions)\n",
    "    except ValueError:\n",
    "        # Happens if only one class is present in the batch, handle gracefully\n",
    "        auroc, auprc = 0.5, 0.5\n",
    "        \n",
    "    model.train()\n",
    "    return avg_loss, auroc, auprc\n",
    "\n",
    "class EarlyStopping:\n",
    "    \"\"\"Stops training if validation loss doesn't improve after a given patience.\"\"\"\n",
    "    def __init__(self, patience=10, min_delta=0, path='best_checkpoint.pt'):\n",
    "        self.patience = patience\n",
    "        self.min_delta = min_delta\n",
    "        self.counter = 0\n",
    "        self.best_loss = np.inf\n",
    "        self.early_stop = False\n",
    "        self.path = path\n",
    "\n",
    "    def __call__(self, val_loss, model):\n",
    "        if val_loss < self.best_loss - self.min_delta:\n",
    "            self.best_loss = val_loss\n",
    "            self.counter = 0\n",
    "            # Save the best model state\n",
    "            torch.save(model.state_dict(), self.path)\n",
    "        elif val_loss > self.best_loss + self.min_delta:\n",
    "            self.counter += 1\n",
    "            if self.counter >= self.patience:\n",
    "                self.early_stop = True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee252eef-1f3e-4641-89bf-e4b7f6387b87",
   "metadata": {},
   "source": [
    "### 3. Main Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6b41825-cd16-47ab-896c-c064a177f590",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, train_loader, val_loader, criterion, optimizer, scheduler, num_epochs, checkpoint_path='dti_model_best.pt'):\n",
    "    \"\"\"Main function to run the training process.\"\"\"\n",
    "    \n",
    "    print(f\"\\n4. Starting Training on device: {DEVICE}\")\n",
    "    print(f\"   Model parameters: {sum(p.numel() for p in model.parameters() if p.requires_grad):,}\")\n",
    "    \n",
    "    # Initialize Early Stopping\n",
    "    early_stopper = EarlyStopping(patience=10, min_delta=0.001, path=checkpoint_path)\n",
    "    \n",
    "    for epoch in range(1, num_epochs + 1):\n",
    "        model.train()\n",
    "        t0 = time.time()\n",
    "        train_loss = 0\n",
    "        \n",
    "        # --- Training Step ---\n",
    "        for drug_batch, target_batch, labels in train_loader:\n",
    "            drug_batch = drug_batch.to(DEVICE)\n",
    "            target_batch = target_batch.to(DEVICE)\n",
    "            labels = labels.to(DEVICE).float().unsqueeze(1)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            predictions = model(drug_batch, target_batch)\n",
    "            loss = criterion(predictions, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            train_loss += loss.item() * len(labels)\n",
    "            \n",
    "        avg_train_loss = train_loss / len(train_loader.dataset)\n",
    "        \n",
    "        # --- Validation Step ---\n",
    "        val_loss, val_auroc, val_auprc = evaluate_model(model, val_loader, criterion)\n",
    "        \n",
    "        # --- Optimization Steps ---\n",
    "        scheduler.step(val_loss) # Update learning rate scheduler\n",
    "        early_stopper(val_loss, model) # Check for early stopping and save best model\n",
    "        \n",
    "        t1 = time.time()\n",
    "        \n",
    "        print(f\"Epoch {epoch:02d} | Time: {t1-t0:.2f}s | Train Loss: {avg_train_loss:.4f} | Val Loss: {val_loss:.4f} | Val AUROC: {val_auroc:.4f} | Val AUPRC: {val_auprc:.4f}\")\n",
    "        \n",
    "        if early_stopper.early_stop:\n",
    "            print(f\"ðŸ›‘ Early stopping triggered. Loading best model from epoch {epoch - early_stopper.patience}.\")\n",
    "            # Load the best weights before exiting\n",
    "            model.load_state_dict(torch.load(checkpoint_path))\n",
    "            break\n",
    "            \n",
    "    print(\"Training finished.\")\n",
    "    return model\n",
    "\n",
    "# --- FINAL EXECUTION (REQUIRES DUMMY DATA/MODEL TO BE REPLACED) ---\n",
    "# model, criterion, optimizer, scheduler = initialize_training_components(\n",
    "#     DRUG_IN_FEAT, TARGET_IN_FEAT, EMBEDDING_DIM, HIDDEN_DIM, GNN_LAYERS, CNN_KERNEL_SIZE, LEARNING_RATE\n",
    "# )\n",
    "\n",
    "# # Pass the model to the training function\n",
    "# final_best_model = train_model(\n",
    "#     model, train_loader, val_loader, criterion, optimizer, scheduler, NUM_EPOCHS\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f45910d-5b08-4047-83ff-9b28580c5478",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
